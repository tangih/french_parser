{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequoia dataset\n",
    "dataset = []\n",
    "with open('sequoia-corpus+fct.mrg_strict', 'r') as f:\n",
    "    for line in f:\n",
    "        dataset.append(line[:-1])\n",
    "n_samples = len(dataset)\n",
    "random.shuffle(dataset)\n",
    "train_set = dataset[:int(0.8*n_samples)]\n",
    "valid_set = dataset[int(0.8*n_samples): int(0.9*n_samples)]\n",
    "test_set = dataset[int(0.9*n_samples):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    this function parses the input line from the sequoia dataset\n",
    "    ------------------------------------------------------------------\n",
    "    returns: \n",
    "            sentence: the input raw line transformed to a sentence\n",
    "        \n",
    "               vocab: a list of tuple (word, pos) associated to the vocabulary of the given sentence, \n",
    "        unwrap_rules: a list of list containing the rules associated to the sentence\n",
    "    \"\"\"\n",
    "    prev = 0\n",
    "    sentence = ''\n",
    "    read_pos = False\n",
    "    pos = ''\n",
    "    vocab = []\n",
    "    rules = []\n",
    "    level = 0\n",
    "    for i in range(len(line)):\n",
    "        c = line[i]\n",
    "        if read_pos:\n",
    "            if c == ' ':\n",
    "                read_pos = False\n",
    "                pos = pos.split('-')[0]\n",
    "                if level-2 >= 0:\n",
    "                    rules[level-2][-1].append(pos)\n",
    "                rules[level-1][-1].append(pos)\n",
    "            else:\n",
    "                pos += c\n",
    "        if c == '(':\n",
    "            # in that case, we start reading a symbol\n",
    "            level += 1\n",
    "            if len(rules) <= level - 1:\n",
    "                rules.append([])\n",
    "            rules[level-1].append([])\n",
    "            read_pos = True\n",
    "            pos = ''\n",
    "        if c == ')':\n",
    "            # in that case, we just read a terminal symbol\n",
    "            level -= 1\n",
    "            if line[i-1] != ')':\n",
    "                word = line[prev+1:i]\n",
    "                sentence += word + ' '\n",
    "                pos = pos.split('-')[0]\n",
    "                rules[level][-1].append(pos.lower())\n",
    "                vocab.append((word, pos))\n",
    "        if c == ' ':\n",
    "            prev = i\n",
    "    sentence = sentence[:-1]\n",
    "    unwrap_rules = []\n",
    "    for level in range(len(rules)):\n",
    "        for i in range(len(rules[level])):\n",
    "            unwrap_rules.append(rules[level][i])\n",
    "    return sentence, vocab, unwrap_rules[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pcfg(train_set):\n",
    "    \"\"\"\n",
    "    creates a PCFG from the input training set\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Returns:\n",
    "        heads:\n",
    "            the list of non terminal symbols\n",
    "        rules:\n",
    "            for each symbol in `heads` at index i, `rules[i]` is the list of rules in the CFG\n",
    "            associated to the head\n",
    "        freqs_pos:\n",
    "            for each symbol in `heads` at index i, `freqs_pos[i]` is the list of probabilities\n",
    "            associated to `rules[i]` in the PCFG\n",
    "        words:\n",
    "            for each symbol in `heads` at index i, `words[i]` is the list of terminal symbols \n",
    "            associated to the PoS (since all symbols in heads are not necessarely PoS, this\n",
    "            list may be empty)\n",
    "        freqs_word:\n",
    "            the frequency associated to the words in the lexicon `word`\n",
    "    \"\"\"\n",
    "    rule_counts = {}\n",
    "    sentences = []\n",
    "    rule_string = {}\n",
    "    lexicon = {}\n",
    "    for line in train_set:\n",
    "        # parses every line in the training set, then fill the frequency dictionaries in order\n",
    "        # to count the occurences of the rules contained in the lines, and the frequencies of \n",
    "        # each word in the obtained lexicon\n",
    "        sentence, vocab, rules = parse_line(line)\n",
    "        sentences.append(sentence)\n",
    "        for tup in vocab:\n",
    "            word, pos = tup\n",
    "            if pos not in lexicon:\n",
    "                lexicon[pos] = {}\n",
    "            if word not in lexicon[pos]:\n",
    "                lexicon[pos][word] = 0\n",
    "            lexicon[pos][word] += 1\n",
    "        for rule in rules:\n",
    "            head = rule[0]\n",
    "            rule = rule[1:]\n",
    "            if head not in rule_counts:\n",
    "                rule_counts[head] = {}\n",
    "            rule_str = '_'.join(rule)\n",
    "            if rule_str not in rule_counts[head]:\n",
    "                rule_counts[head][rule_str] = 0\n",
    "            rule_counts[head][rule_str] += 1\n",
    "\n",
    "    # from the computed counts, compute the frequencies of the rules\n",
    "    heads = []\n",
    "    rules = []\n",
    "    freqs_pos = []\n",
    "    for i, head in enumerate(rule_counts.keys()):\n",
    "        heads.append(head)\n",
    "        rules.append([])\n",
    "        freqs_pos.append([])\n",
    "        s = sum(rule_counts[head].values())\n",
    "        for j, rule_str in enumerate(rule_counts[head].keys()):\n",
    "            rule = rule_str.split('_')\n",
    "            rules[i].append(rule)\n",
    "            freqs_pos[i].append(rule_counts[head][rule_str] / s)\n",
    "            \n",
    "    # from the computed counts, compute frequencies associated to the lexicon\n",
    "    words = [[] for _ in range(len(heads))]\n",
    "    freqs_word = [[] for _ in range(len(heads))]\n",
    "    for pos in lexicon.keys():\n",
    "        i = heads.index(pos)\n",
    "        s = sum(lexicon[pos].values())\n",
    "        for j, word in enumerate(lexicon[pos].keys()):\n",
    "            words[i].append(word)\n",
    "            freqs_word[i].append(lexicon[pos][word] / s)\n",
    "    return heads, rules, freqs_pos, words, freqs_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads, rules, freqs_pos, words, freqs_word = create_pcfg(train_set)\n",
    "n_h = 4\n",
    "# print(heads[n_h])\n",
    "# print(list(zip(freqs[n_h], rules[n_h])))\n",
    "# print(sum(freqs[n_h]))\n",
    "# print(list(zip(words[n_h], freqs_word[n_h])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
