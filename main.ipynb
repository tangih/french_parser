{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequoia dataset\n",
    "dataset = []\n",
    "with open('sequoia-corpus+fct.mrg_strict', 'r') as f:\n",
    "    for line in f:\n",
    "        dataset.append(line[:-1])\n",
    "n_samples = len(dataset)\n",
    "random.shuffle(dataset)\n",
    "train_set = dataset[:int(0.8*n_samples)]\n",
    "valid_set = dataset[int(0.8*n_samples): int(0.9*n_samples)]\n",
    "test_set = dataset[int(0.9*n_samples):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    parses the input line from the sequoia dataset\n",
    "    ------------------------------------------------------------------\n",
    "    returns: \n",
    "            sentence: the input raw line transformed to a sentence\n",
    "        \n",
    "               vocab: a list of tuple (word, pos) associated to the vocabulary of the given sentence, \n",
    "        unwrap_rules: a list of list containing the rules associated to the sentence\n",
    "    \"\"\"\n",
    "    prev = 0\n",
    "    sentence = ''\n",
    "    read_pos = False\n",
    "    pos = ''\n",
    "    vocab = []\n",
    "    rules = []\n",
    "    level = 0\n",
    "    for i in range(len(line)):\n",
    "        c = line[i]\n",
    "        if read_pos:\n",
    "            if c == ' ':\n",
    "                read_pos = False\n",
    "                pos = pos.split('-')[0]\n",
    "                if level-2 >= 0:\n",
    "                    rules[level-2][-1].append(pos)\n",
    "                rules[level-1][-1].append(pos)\n",
    "            else:\n",
    "                pos += c\n",
    "        if c == '(':\n",
    "            # in that case, we start reading a symbol\n",
    "            level += 1\n",
    "            if len(rules) <= level - 1:\n",
    "                rules.append([])\n",
    "            rules[level-1].append([])\n",
    "            read_pos = True\n",
    "            pos = ''\n",
    "        if c == ')':\n",
    "            # in that case, we just read a terminal symbol\n",
    "            level -= 1\n",
    "            if line[i-1] != ')':\n",
    "                word = line[prev+1:i]\n",
    "                sentence += word + ' '\n",
    "                pos = pos.split('-')[0]\n",
    "                rules[level][-1].append(pos.lower())\n",
    "                vocab.append((word, pos))\n",
    "        if c == ' ':\n",
    "            prev = i\n",
    "    sentence = sentence[:-1]\n",
    "    unwrap_rules = []\n",
    "    for level in range(len(rules)):\n",
    "        for i in range(len(rules[level])):\n",
    "            unwrap_rules.append(rules[level][i])\n",
    "    return sentence, vocab, unwrap_rules[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pcfg(train_set):\n",
    "    \"\"\"\n",
    "    creates a PCFG from the input training set\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Returns:\n",
    "        heads:\n",
    "            the list of non terminal symbols\n",
    "        rules:\n",
    "            for each symbol in `heads` at index i, `rules[i]` is the list of rules in the CFG\n",
    "            associated to the head\n",
    "        freqs_pos:\n",
    "            for each symbol in `heads` at index i, `freqs_pos[i]` is the list of probabilities\n",
    "            associated to `rules[i]` in the PCFG\n",
    "        words:\n",
    "            for each symbol in `heads` at index i, `words[i]` is the list of terminal symbols \n",
    "            associated to the PoS (since all symbols in heads are not necessarely PoS, this\n",
    "            list may be empty)\n",
    "        freqs_word:\n",
    "            the frequency associated to the words in the lexicon `word`\n",
    "    \"\"\"\n",
    "    rule_counts = {}\n",
    "    sentences = []\n",
    "    rule_string = {}\n",
    "    lexicon = {}\n",
    "    for line in train_set:\n",
    "        # parses every line in the training set, then fill the frequency dictionaries in order\n",
    "        # to count the occurences of the rules contained in the lines, and the frequencies of \n",
    "        # each word in the obtained lexicon\n",
    "        sentence, vocab, rules = parse_line(line)\n",
    "        sentences.append(sentence)\n",
    "        for tup in vocab:\n",
    "            word, pos = tup\n",
    "            if pos not in lexicon:\n",
    "                lexicon[pos] = {}\n",
    "            if word not in lexicon[pos]:\n",
    "                lexicon[pos][word] = 0\n",
    "            lexicon[pos][word] += 1\n",
    "        for rule in rules:\n",
    "            head = rule[0]\n",
    "            rule = rule[1:]\n",
    "            if head not in rule_counts:\n",
    "                rule_counts[head] = {}\n",
    "            rule_str = '_'.join(rule)\n",
    "            if rule_str not in rule_counts[head]:\n",
    "                rule_counts[head][rule_str] = 0\n",
    "            rule_counts[head][rule_str] += 1\n",
    "\n",
    "    # from the computed counts, compute the frequencies of the rules\n",
    "    heads = []\n",
    "    rules = []\n",
    "    freqs_pos = []\n",
    "    for i, head in enumerate(rule_counts.keys()):\n",
    "        heads.append(head)\n",
    "        rules.append([])\n",
    "        freqs_pos.append([])\n",
    "        s = sum(rule_counts[head].values())\n",
    "        for j, rule_str in enumerate(rule_counts[head].keys()):\n",
    "            rule = rule_str.split('_')\n",
    "            rules[i].append(rule)\n",
    "            freqs_pos[i].append(rule_counts[head][rule_str] / s)\n",
    "            \n",
    "    # from the computed counts, compute frequencies associated to the lexicon\n",
    "    words = [[] for _ in range(len(heads))]\n",
    "    freqs_word = [[] for _ in range(len(heads))]\n",
    "    for pos in lexicon.keys():\n",
    "        i = heads.index(pos)\n",
    "        s = sum(lexicon[pos].values())\n",
    "        for j, word in enumerate(lexicon[pos].keys()):\n",
    "            words[i].append(word)\n",
    "            freqs_word[i].append(lexicon[pos][word] / s)\n",
    "    return heads, rules, freqs_pos, words, freqs_word, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads, rules, freqs_pos, words, freqs_word, sentences = create_pcfg(train_set)\n",
    "n_h = 4\n",
    "# print(heads[n_h])\n",
    "# print(list(zip(freqs[n_h], rules[n_h])))\n",
    "# print(sum(freqs[n_h]))\n",
    "# print(list(zip(words[n_h], freqs_word[n_h])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(words):\n",
    "    vocab = []\n",
    "    for word_list in words:\n",
    "        for word in word_list:\n",
    "            vocab.append(word)\n",
    "    vocab.append('/////START/////')\n",
    "    return vocab\n",
    "\n",
    "def unigram(train_sentence_list, add_smoothing=0):\n",
    "    occur = {'/////START/////': 0}\n",
    "    count = 0\n",
    "    for sentence in train_sentence_list:\n",
    "        sample = sentence.split(' ')\n",
    "        occur['/////START/////'] += 1\n",
    "        count += 1\n",
    "        for word in sample:\n",
    "            if word not in occur:\n",
    "                occur[word] = 0\n",
    "            occur[word] += 1\n",
    "            count += 1\n",
    "    return occur, count\n",
    "\n",
    "def bigram(label_set, train_sentence_list):\n",
    "    occur = [{} for _ in label_set] + [{}]\n",
    "    count = 0\n",
    "    for sentence in train_sentence_list:\n",
    "        sample = sentence.split(' ')\n",
    "        word = label_set.index(sample[0])\n",
    "        if word not in occur[-1]:\n",
    "            occur[-1][word] = 0\n",
    "        occur[-1][word] += 1\n",
    "        count += 1\n",
    "        for i in range(0, len(sample)-1):\n",
    "            id_word1 = label_set.index(sample[i])\n",
    "            word2 = sample[i+1]\n",
    "            if word2 not in occur[id_word1]:\n",
    "                occur[id_word1][word2] = 0\n",
    "            occur[id_word1][word2] += 1\n",
    "            count += 1\n",
    "    return occur, count\n",
    "\n",
    "vocab = get_vocab(words)\n",
    "unigram, n_unigram = unigram(sentences)\n",
    "bigram, n_bigram = bigram(vocab, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_li(w1, w2, unigram, bigram, lam, n_unigram):\n",
    "    label_set = list(unigram.keys())\n",
    "    id_w1 = label_set.index(w1)\n",
    "    unigram_count = unigram[w1]\n",
    "    assert unigram_count > 0, 'Error, no occurence of {} was found'.format(w1)\n",
    "    bigram_count = 0 if w2 not in bigram[id_w1] else bigram[id_w1][w2]\n",
    "    p_mle = bigram_count / unigram_count\n",
    "    p_uni = unigram[w2] / n_unigram\n",
    "    return lam * p_uni + (1 - lam) * p_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans un rapport de mars 1999 , la police a estimé qu' au_total , les salaires versés par la ville de Paris au RPR concernant les emplois fictifs avoisinent 30 millions de francs .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.981408671209237e-05"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def candidates(word, vocab, max_dist=2):\n",
    "    candidates = []\n",
    "    probs = []\n",
    "    for candidate in vocab:\n",
    "        dist, prob = levenshtein_dist(word, candidate)\n",
    "        if dist <= max_dist:\n",
    "            candidates.append(candidate)\n",
    "            probs.append(prob)\n",
    "    return candidates, probs\n",
    "\n",
    "def oov(sentence, index, unigram, bigram, n_unigram):\n",
    "    sentence = ['/////START/////'] + sentence\n",
    "    lam = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in order to compute the probability associated to a spelling mistake \n",
    "\"\"\"\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz@'\n",
    "\n",
    "# DEL[X, Y] = deletion of Y after X\n",
    "DEL = np.array([0, 7, 58, 21, 3, 5, 18, 8, 61, 0, 4, 43, 5, 53, 0, 9, 0, 98, 28, 53, 62, 1, 0, 0, 2, 0, 2, 2,\n",
    "                1, 0, 22, 0, 0, 0, 183, 0, 0, 26, 0, 0, 2, 0, 0, 6, 17, 0, 6, 1, 0, 0, 0, 0, 37, 0, 70, 0, 63, \n",
    "                0, 0, 24, 320, 0, 9, 17, 0, 0, 33, 0, 0, 46, 6, 54, 17, 0, 0, 0, 1, 0, 12, 0, 7, 25, 45, 0, \n",
    "                10, 0, 62, 1, 1, 8, 4, 3, 3, 0, 0, 11, 1, 0, 3, 2, 0, 0, 6, 0, 80, 1, 50, 74, 89, 3, 1, 1, 6, \n",
    "                0, 0, 32, 9, 76, 19, 9, 1, 237, 223, 34, 8, 2, 1, 7, 1, 0, 4, 0, 0, 0, 13, 46, 0, 0, 79, 0, 0,\n",
    "                12, 0, 0, 4, 0, 0, 11, 0, 8, 1, 0, 0, 0, 1, 0, 25, 0, 0, 2, 83, 1, 37, 25, 39, 0, 0, 3, 0, 29,\n",
    "                4, 0, 0, 52, 7, 1, 22, 0, 0, 0, 1, 0, 15, 12, 1, 3, 20, 0, 0, 25, 24, 0, 0, 7, 1, 9, 22, 0, 0,\n",
    "                15, 1, 26, 0, 0, 1, 0, 1, 0, 26, 1, 60, 26, 23, 1, 9, 0, 1, 0, 0, 38, 14, 82, 41, 7, 0, 16, 71,\n",
    "                64, 1, 1, 0, 0, 1, 7, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, \n",
    "                0, 0, 4, 0, 0, 1, 15, 1, 8, 1, 5, 0, 1, 3, 0, 17, 0, 0, 0, 1, 5, 0, 0, 0, 1, 0, 0, 0, 24, 0, 1, \n",
    "                6, 48, 0, 0, 0, 217, 0, 0, 211, 2, 0, 29, 0, 0, 2, 12, 7, 3, 2, 0, 0, 11, 0, 15, 10, 0, 0, 33, \n",
    "                0, 0, 1, 42, 0, 0, 0, 180, 7, 7, 31, 0, 0, 9, 0, 4, 0, 0, 0, 0, 0, 21, 0, 42, 71, 68, 1, 160,\n",
    "                0, 191, 0, 0, 0, 17, 144, 21, 0, 0, 0, 127, 87, 43, 1, 1, 0, 2, 0, 11, 4, 3, 6, 8, 0, 5, 0, 4,\n",
    "                1, 0, 13, 9, 70, 26, 20, 0, 98, 20, 13, 47, 2, 5, 0, 1, 0, 25, 0, 0, 0, 22, 0, 0, 12, 15, 0, 0,\n",
    "                28, 1, 0, 30, 93, 0, 58, 1, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 63, 4, 12, 19, 188, 0, 11, 5, 132, 0, 3, 33, 7, 157, 21, 2,\n",
    "                0, 277, 103, 68, 0, 10, 1, 0, 27, 0, 16, 0, 27, 0, 74, 1, 0, 18, 231, 0, 0, 2, 1, 0, 30, 30, \n",
    "                0, 4, 265, 124, 21, 0, 0, 0, 1, 0, 24, 1, 2, 0, 76, 1, 7, 49, 427, 0, 0, 31, 3, 3, 11, 1, 0, \n",
    "                203, 5, 137, 14, 0, 4, 0, 2, 0, 26, 6, 9, 10, 15, 0, 1, 0, 28, 0, 0, 39, 2, 111, 1, 0, 0, 129,\n",
    "                31, 66, 0, 0, 0, 0, 1, 0, 9, 0, 0, 0, 58, 0, 0, 0, 31, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, \n",
    "                0, 0, 1, 0, 40, 0, 0, 1, 11, 1, 0, 11, 15, 0, 0, 1, 0, 2, 2, 0, 0, 2, 24, 0, 0, 0, 0, 0, 0, 0,\n",
    "                1, 0, 17, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 5, 0, 0, 0, 0, 1, 0, 2, 1, 34, 0, 2,\n",
    "                0, 1, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 0, 17, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, \n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 20, 14, 41, 31, 20, 20, 7, 6, 20, 3, 6, 22, 16,\n",
    "                5, 5, 17, 0, 28, 26, 6, 2, 1, 24, 0, 0, 2]).reshape((27, 26))\n",
    "# ADD[X, Y] = insertion of Y after X\n",
    "ADD = np.array([15, 1, 14, 7, 10, 0, 1, 1, 33, 1, 4, 31, 2, 39, 12, 4, 3, 28, 134, 7, 28, 0, 1, 1, 4, 1, 3, 11,\n",
    "                0, 0, 7, 0, 1, 0, 50, 0, 0, 15, 0, 1, 1, 0, 0, 5, 16, 0, 0, 3, 0, 0, 0, 0, 19, 0, 54, 1, 13, 0,\n",
    "                0, 18, 50, 0, 3, 1, 1, 1, 7, 1, 0, 7, 25, 7, 8, 4, 0, 1, 0, 0, 18, 0, 3, 17, 14, 2, 0, 0, 9, 0,\n",
    "                0, 6, 1, 9, 13, 0, 0, 6, 119, 0, 0, 0, 0, 0, 5, 0, 39, 2, 8, 76, 147, 2, 0, 1, 4, 0, 3, 4, 6, \n",
    "                27, 5, 1, 0, 83, 417, 6, 4, 1, 10, 2, 8, 0, 1, 0, 0, 0, 2, 27, 1, 0, 12, 0, 0, 10, 0, 0, 0, 0,\n",
    "                0, 5, 23, 0, 1, 0, 0, 0, 1, 0, 8, 0, 0, 0, 5, 1, 5, 12, 8, 0, 0, 2, 0, 1, 1, 0, 1, 5, 69, 2, 3,\n",
    "                0, 1, 0, 0, 0, 4, 1, 0, 1, 24, 0, 10, 18, 17, 2, 0, 1, 0, 1, 4, 0, 0, 16, 24, 22, 1, 0, 5, 0,\n",
    "                3, 0, 10, 3, 13, 13, 25, 0, 1, 1, 69, 2, 1, 17, 11, 33, 27, 1, 0, 9, 30, 29, 11, 0, 0, 1, 0, 1,\n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 4, 0, 1, 9, 0,\n",
    "                0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0, 0, 95, 0, 1, 0, 0, 0, 4, 0, 3, 1, 0, 1, 38, 0, 0, 0, 79, 0, 2,\n",
    "                128, 1, 0, 7, 0, 0, 0, 97, 7, 3, 1, 0, 0, 2, 0, 11, 1, 1, 0, 17, 0, 0, 1, 6, 0, 1, 0, 102, 44,\n",
    "                7, 2, 0, 0, 47, 1, 2, 0, 1, 0, 0, 0, 15, 5, 7, 13, 52, 4, 17, 0, 34, 0, 1, 1, 26, 99, 12, 0, 0,\n",
    "                2, 156, 53, 1, 1, 0, 0, 1, 0, 14, 1, 1, 3, 7, 2, 1, 0, 28, 1, 0, 6, 3, 13, 64, 30, 0, 16, 59, \n",
    "                4, 19, 1, 0, 0, 1, 1, 23, 0, 1, 1, 10, 0, 0, 20, 3, 0, 0, 2, 0, 0, 26, 70, 0, 29, 52, 9, 1, 1,\n",
    "                1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 15, 2,\n",
    "                1, 0, 89, 1, 1, 2, 64, 0, 0, 5, 9, 7, 10, 0, 0, 132, 273, 29, 7, 0, 1, 0, 10, 0, 13, 1, 7, 20,\n",
    "                41, 0, 1, 50, 101, 0, 2, 2, 10, 7, 3, 1, 0, 1, 205, 49, 7, 0, 1, 0, 7, 0, 39, 0, 0, 3, 65, 1,\n",
    "                10, 24, 59, 1, 0, 6, 3, 1, 23, 1, 0, 54, 264, 183, 11, 0, 5, 0, 6, 0, 15, 0, 3, 0, 9, 0, 0, 1,\n",
    "                24, 1, 1, 3, 3, 9, 1, 3, 0, 49, 19, 27, 26, 0, 0, 2, 3, 0, 0, 2, 0, 0, 36, 0, 0, 0, 10, 0, 0,\n",
    "                1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 5, 1, 0, 0, 0, 0, 0, 0, 1, 10, 0, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 1,\n",
    "                1, 8, 0, 2, 0, 4, 0, 0, 0, 0, 0, 18, 0, 1, 0, 0, 6, 1, 0, 0, 0, 1, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0,\n",
    "                1, 0, 0, 5, 1, 2, 0, 3, 0, 0, 0, 2, 0, 0, 1, 1, 6, 0, 0, 0, 1, 33, 1, 13, 0, 1, 0, 2, 0, 2, 0, \n",
    "                0, 0, 5, 1, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 46, 8, 9, 8, 26, 11, 14,\n",
    "                3, 5, 1, 17, 5, 6, 2, 2, 10, 0, 6, 23, 2, 11, 1, 2, 1, 1, 2]).reshape((27, 26))\n",
    "\n",
    "# SUB[X, Y] = substitution of X (correct) for Y (incorrect)\n",
    "SUB = np.array([0, 0, 7, 1, 342, 0, 0, 2, 118, 0, 1, 0, 0, 3, 76, 0, 0, 1, 35, 9, 9, 0, 1, 0, 5, 0, 0, 0, 9, 9,\n",
    "                2, 2, 3, 1, 0, 0, 0, 5, 11, 5, 0, 10, 0, 0, 2, 1, 0, 0, 8, 0, 0, 0, 6, 5, 0, 16, 0, 9, 5, 0, 0,\n",
    "                0, 1, 0, 7, 9, 1, 10, 2, 5, 39, 40, 1, 3, 7, 1, 1, 0, 1, 10, 13, 0, 12, 0, 5, 5, 0, 0, 2, 3, 7,\n",
    "                3, 0, 1, 0, 43, 30, 22, 0, 0, 4, 0, 2, 0, 388, 0, 3, 11, 0, 2, 2, 0, 89, 0, 0, 3, 0, 5, 93, 0, \n",
    "                0, 14, 12, 6, 15, 0, 1, 0, 18, 0, 0, 15, 0, 3, 1, 0, 5, 2, 0, 0, 0, 3, 4, 1, 0, 0, 0, 6, 4, 12,\n",
    "                0, 0, 2, 0, 0, 0, 4, 1, 11, 11, 9, 2, 0, 0, 0, 1, 1, 3, 0, 0, 2, 1, 3, 5, 13, 21, 0, 0, 1, 0,\n",
    "                3, 0, 1, 8, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 12, 14, 2, 3, 0, 3, 1, 11, 0, 0, 2, 0, 0, 0, 103, 0,\n",
    "                0, 0, 146, 0, 1, 0, 0, 0, 0, 6, 0, 0, 49, 0, 0, 0, 2, 1, 47, 0, 2, 1, 15, 0, 0, 1, 1, 9, 0, 0,\n",
    "                1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 2, 8, 4, 1, 1, 2, 5, 0, 0, 0, 0,\n",
    "                5, 0, 2, 0, 0, 0, 6, 0, 0, 0, .4, 0, 0, 3, 2, 10, 1, 4, 0, 4, 5, 6, 13, 0, 1, 0, 0, 14, 2, 5,\n",
    "                0, 11, 10, 2, 0, 0, 0, 0, 0, 0, 1, 3, 7, 8, 0, 2, 0, 6, 0, 0, 4, 4, 0, 180, 0, 6, 0, 0, 9, 15, \n",
    "                13, 3, 2, 2, 3, 0, 2, 7, 6, 5, 3, 0, 1, 19, 1, 0, 4, 35, 78, 0, 0, 7, 0, 28, 5, 7, 0, 0, 1, 2,\n",
    "                0, 2, 91, 1, 1, 3, 116, 0, 0, 0, 25, 0, 2, 0, 0, 0, 0, 14, 0, 2, 4, 14, 39, 0, 0, 0, 18, 0, 0,\n",
    "                11, 1, 2, 0, 6, 5, 0, 2, 9, 0, 2, 7, 6, 15, 0, 0, 1, 3, 6, 0, 4, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
    "                27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 30, 12, 2, 2, 8, 2, 0,\n",
    "                5, 8, 4, 20, 1, 14, 0, 0, 12, 22, 4, 0, 0, 1, 0, 0, 11, 8, 27, 33, 35, 4, 0, 1, 0, 1, 0, 27, 0,\n",
    "                6, 1, 7, 0, 14, 0, 15, 0, 0, 5, 3, 20, 1, 3, 4, 9, 42, 7, 5, 19, 5, 0, 1, 0, 14, 9, 5, 5, 6, 0,\n",
    "                11, 37, 0, 0, 2, 19, 0, 7, 6, 20, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 2, 43, 0, 0, 4, 0, 0, \n",
    "                0, 0, 2, 0, 8, 0, 0, 0, 7, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 8, 3, 0, 0, 0, 0, 0, 0, \n",
    "                2, 2, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 7, 0, 6, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 15, 0, 1, 7, 15, 0, 0, \n",
    "                0, 2, 0, 6, 1, 0, 7, 36, 8, 5, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 7, 5, 0, 0, 0, 0,\n",
    "                2, 21, 3, 0, 0, 0, 0, 3, 0]).reshape((26, 26))\n",
    "\n",
    "# REV[X, Y] = reversal of XY\n",
    "REV = np.array([0, 0, 2, 1, 1, 0, 0, 0, 19, 0, 1, 14, 4, 25, 10, 3, 0, 27, 3, 5, 31, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
    "                85, 0, 0, 15, 0, 0, 13, 0, 0, 0, 3, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 4, 5, 0, 0, 0, 0, 60, 0, 0, 21, 6, 16, 11, 2,\n",
    "                0, 29, 5, 0, 85, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 15, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0,\n",
    "                0, 12, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 15, 8, 31, \n",
    "                3, 66, 1, 3, 0, 0, 0, 0, 9, 0, 5, 11, 0, 1, 13, 42, 35, 0, 6, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 12, 20, 0, 1, 0, 4, 0, 0, 0, 0, 0, 1, 3, 0,\n",
    "                0, 1, 1, 3, 9, 0, 0, 7, 0, 9, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0,\n",
    "                0, 0, 0, 0, 15, 0, 6, 2, 12, 0, 8, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 6, 4, 0, 0, 0, 0, 0, 0, 5,\n",
    "                0, 2, 0, 4, 0, 0, 0, 5, 0, 0, 1, 0, 5, 0, 1, 0, 11, 1, 1, 0, 0, 7, 1, 0, 0, 17, 0, 0, 0, 4, 0,\n",
    "                0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 24, 0, 3, 0, 14, 0, 2, 2, 0, 7, 30, \n",
    "                1, 0, 0, 0, 2, 10, 0, 0, 0, 2, 0, 4, 0, 0, 0, 9, 0, 0, 5, 15, 0, 0, 5, 2, 0, 1, 22, 0, 0, 0, 1,\n",
    "                3, 0, 0, 0, 16, 0, 4, 0, 3, 0, 4, 0, 0, 21, 49, 0, 0, 4, 0, 0, 3, 0, 0, 5, 0, 0, 11, 0, 2, 0,\n",
    "                0, 0, 22, 0, 5, 1, 1, 0, 2, 0, 2, 0, 0, 2, 1, 0, 20, 2, 0, 11, 11, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4,\n",
    "                0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 1, 10, \n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                0]).reshape((26, 26))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(t|c) = \\left\\{\n",
    " \\begin{array}{ll}\n",
    " \\frac{\\text{del}(c_{p-1}, c_p)}{\\text{chars}(c_{p-1}, c_p)} & \\text{if deletion}\\\\\n",
    " \\frac{\\text{add}(c_{p-1}, t_p)}{\\text{chars}(c_{p-1})} & \\text{if insertion}\\\\\n",
    " \\frac{\\text{sub}(t_{p}, c_p)}{\\text{chars}(c_p)} & \\text{if substitution}\\\\\n",
    " \\frac{\\text{rev}(c_{p}, c_{p+1})}{\\text{chars}(c_{p}, c_{p+1})} & \\text{if reversal}\n",
    " \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-bc1e2ddaef2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevenshtein_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'across'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdele\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-184-bc1e2ddaef2b>\u001b[0m in \u001b[0;36mlevenshtein_dist\u001b[0;34m(s1, s2, dele, add, sub)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;31m# insertion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# substitution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "def compute_chars(sentences):\n",
    "    \"\"\"\n",
    "    compute approximate of `chars` values used in the [Kernighan, Church, Gale, '90] paper\n",
    "    \"\"\"\n",
    "    uni_char = np.zeros(len(alphabet), dtype=np.float)\n",
    "    bi_char = np.zeros((len(alphabet), len(alphabet)), dtype=np.float)\n",
    "    N = 44e6  # original dataset contains 44 million words\n",
    "    n_words = 0\n",
    "    for sentence in sentences:\n",
    "        word_list = sentence.split(' ')\n",
    "        for word in word_list:\n",
    "            n_words += 1\n",
    "            for i in range(-1, len(word)):\n",
    "                x = word[i] if i > 0 else '@'\n",
    "                try:\n",
    "                    id_x = alphabet.index(x)\n",
    "                    uni_char[id_x] += 1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if i < len(word)-1:\n",
    "                    y = word[i+1]\n",
    "                    try:\n",
    "                        id_y = alphabet.index(y)\n",
    "                        bi_char[id_x, id_y] += 1\n",
    "                    except ValueError:\n",
    "                        continue   \n",
    "    return uni_char * (N / n_words), bi_char * (N / n_words)\n",
    "\n",
    "uni_chars, bi_chars = compute_chars(sentences)\n",
    "# use add-1 smoothing\n",
    "DEL = DEL + 1\n",
    "ADD = DEL + 1\n",
    "SUB = DEL + 1\n",
    "uni_chars = uni_chars + 1\n",
    "bi_chars = bi_chars + 1\n",
    "\n",
    "dele = np.zeros_like(DEL)\n",
    "add = np.zeros_like(ADD)\n",
    "sub = np.zeros_like(SUB)\n",
    "for i in range(len(alphabet)):\n",
    "    for j in range(len(alphabet)-1):\n",
    "        dele[i, j] = DEL[i, j] / bi_chars[i, j]\n",
    "        add[i, j] = ADD[i, j] / uni_chars[i]\n",
    "for i in range(len(alphabet)-1):\n",
    "    for j in range(len(alphabet)-1):\n",
    "        sub[i, j] = SUB[i, j] / uni_chars[j]\n",
    "\n",
    "def levenshtein_dist(s1, s2, dele, add, sub):\n",
    "    \"\"\"\n",
    "    Computes Levenshtein distance, between candidate word s1 and original word s2, \n",
    "    with associated probability of error\n",
    "    Note: Levenshtein distance does not take into account reversal, maybe should add it\n",
    "    \"\"\"\n",
    "    m = np.zeros((len(s1)+1, len(s2)+1), dtype=np.int)\n",
    "    p = np.zeros((len(s1)+1, len(s2)+1), dtype=np.float)\n",
    "    for i in range(len(s1)+1):\n",
    "        m[i, 0] = i\n",
    "        # compute probability for deletion\n",
    "        if i == 0:\n",
    "            p[i, 0] = 1\n",
    "        else:\n",
    "            ind = alphabet.index('@')\n",
    "            p[i, 0] = p[i-1, 0] * dele[ind, alphabet.index(s1[i-1])]                      \n",
    "    for j in range(len(s2)+1):\n",
    "        # compute probability for insertion\n",
    "        if j == 0:\n",
    "            p[0, j] = 1\n",
    "        else:\n",
    "            prev_char = '@' if j == 1 else s2[j-2]\n",
    "            p[0, j] = p[0, j-1] * add[alphabet.index(prev_char), \n",
    "                                      alphabet.index(s2[j-1])]\n",
    "        m[0, j] = j\n",
    "    for i in range(1, 1+len(s1)):\n",
    "        for j in range(1, len(s2)+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                k = np.argmin([m[i-1, j] + 1, m[i, j-1] + 1, m[i-1, j-1]])\n",
    "                if k == 0:\n",
    "                    # deletion\n",
    "                    m[i, j] = m[i-1, j] + 1\n",
    "                    prev_char = '@' if j == 1 else s2[j-2]\n",
    "                    p[i, j] = p[i-1, j] * dele[alphabet.index(prev_char), alphabet.index(s1[i-1])]\n",
    "                elif k == 1:\n",
    "                    # insertion\n",
    "                    m[i, j] = m[i, j-1] + 1\n",
    "                    p[i, j] = p[i, j-1] * add[alphabet.index(s2[j-1]), s2[j]]\n",
    "                else:\n",
    "                    # no mistake\n",
    "                    m[i, j] = m[i-1, j-1]\n",
    "                    p[i, j] = p[i-1, j-1]\n",
    "            else:\n",
    "                k = np.argmin([m[i-1, j] + 1, m[i, j-1] + 1, m[i-1, j-1] + 1])\n",
    "                if k == 0:\n",
    "                    # deletion\n",
    "                    m[i, j] = m[i-1, j] + 1\n",
    "                    prev_char = '@' if j == 1 else s2[j-2]\n",
    "                    p[i, j] = p[i-1, j] * dele[alphabet.index(prev_char), alphabet.index(s1[i-1])]\n",
    "                elif k == 1:\n",
    "                    # insertion\n",
    "                    m[i, j] = m[i, j-1] + 1\n",
    "                    p[i, j] = p[i, j-1] * add[alphabet.index(s2[j-1]), s2[j]]\n",
    "                else:\n",
    "                    # substitution\n",
    "                    m[i, j] = m[i-1, j-1]\n",
    "                    p[i, j] = p[i-1, j-1] * sub[alphabet.index(s1[i-1]), alphabet.index([s2[j-1]])]\n",
    "                    # recall that in sub[X, Y], Y is the correct word\n",
    "\n",
    "    return m[len(s1), len(s2)], p[len(s1), len(s2)]\n",
    "\n",
    "print(levenshtein_dist('actress', 'across', dele, add, sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
